# AutoLog

An advanced log analysis platform using LLM for Root Cause Analysis (RCA) and multi-source log integration. Designed to streamline log analysis, provide intelligent insights, and generate comprehensive RCA reports from various log sources including CloudWatch, Splunk, and more.

## 🚀 Features

- **Proactive AI-Driven Monitoring**: Autogenerates alerts before failures—no custom monitors needed
- **LLM-Powered Root Cause Analysis**: Automated, developer-friendly RCA with instant preliminary findings
- **Code-Aware Fix Suggestions**: (Optional) Auto-generates PRs for common issues before a dev even logs in
- **Multi-Source Log Integration**: Connect to CloudWatch, Splunk, and more
- **Intelligent Insights**: AI-driven anomaly detection and pattern recognition
- **Enhanced ML Log Parsing**: 14+ ML algorithms for all log types (JSON, structured, unstructured, mixed)
- **Real-time Log Processing**: Live log ingestion and analysis
- **Role-based Access Control**: Secure, multi-user support
- **Dashboard & Analytics**: Comprehensive reporting and log analytics
- **Modern UI/UX**: Intuitive React interface

## 🤖 Proactive AI-Driven Monitoring & Self-Healing (Why Wait for Outages?)

AutoLog isn’t just a log analyzer—it’s your tireless, ever-vigilant AI SRE sidekick. Here’s what sets it apart:

- **Autogenerated Alerts Before Disaster Strikes:**
  - Constantly watches your logs, dynamically detects emerging error patterns, and fires off alerts *before* your system faceplants.
  - Learns what “normal” looks like, so you don’t have to.

- **Zero Custom Monitors Needed:**
  - Adapts to new log formats, new error types, and new chaos—automatically.

- **Developer-First RCA Pages:**
  - Every alert links to a rich, developer-friendly RCA page. See the error, the context, the likely root cause, and recommended actions—all in one place.
  - Junior devs get a head start, senior devs can validate or override, and everyone saves time.

- **Preliminary RCA, Instantly:**
  - As soon as an anomaly is detected, AutoLog runs a preliminary RCA (Root Cause Analysis) and presents its findings. No more “What happened?”—you’ll know *what*, *where*, and *why*.

- **Code-Aware Fix Suggestions (and PRs!):**
  - With code integration, AutoLog can even suggest (or draft) Pull Requests to fix common issues—sometimes before a human even logs in.

- **Witty, Relentless, and Always On:**
  - AutoLog never sleeps, never gets bored, and never misses a log line. It’s the SRE that doesn’t need coffee.

### Example Workflow

1. **Error Detected:**
   - AutoLog spots a spike in database timeouts at 2am.
2. **Alert Generated:**
   - You get a Slack/Teams/email alert: “Heads up! DB timeouts trending up—preliminary RCA attached.”
3. **RCA Page:**
   - Click the link: see error patterns, root cause, and recommended fixes. (No more log spelunking.)
4. **Fix PR (Optional):**
   - If code support is enabled, AutoLog drafts a PR to bump a config, patch a bug, or add a missing index—ready for your review.
5. **Senior Dev Review:**
   - Senior devs can validate, override, or merge. Junior devs learn from the AI’s reasoning.

> “Why wait for a post-mortem when you can have a pre-mortem?”

AutoLog: Because your logs know the future, and now you do too.

## 🏗️ Architecture

- **Frontend**: React 18 + JavaScript + Vite (with live reload)
- **Backend**: Go + Gin + GORM (normal build)
- **Logparser Microservice**: Python + FastAPI + ML algorithms (port 8001)
- **Database**: PostgreSQL
- **Real-time**: WebSocket support
- **Authentication**: JWT-based authentication
- **Styling**: Tailwind CSS

## 📁 Project Structure

```
autolog/
├── frontend/          # React frontend application
├── backend/           # Go backend API
│   ├── cmd/          # Application entry points
│   ├── internal/     # Private application code
│   ├── pkg/          # Public libraries
│   └── migrations/   # Database migrations
├── logparser_service/ # Python ML logparser microservice
│   ├── enhanced_ml_parser.py  # Enhanced ML parser with 14 algorithms
│   ├── main.py               # FastAPI microservice
│   ├── test_all.py           # Comprehensive test suite
│   └── requirements.txt      # Python dependencies
├── shared/            # Shared types and utilities
├── prd/              # Product Requirements Document
└── docs/             # Project documentation
```

## 🛠️ Setup Instructions

### Prerequisites

- Go 1.24+
- Node.js 18+ 
- Python 3.10+
- PostgreSQL 14+
- npm or yarn

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd autolog
   ```

2. **Install frontend dependencies**
   ```bash
   npm run install:all
   ```

3. **Install logparser microservice dependencies**
   ```bash
   cd logparser_service
   pip install -r requirements.txt
   cd ..
   ```

4. **Environment Setup**
   ```bash
   # Copy environment files
   cp backend/.env.example backend/.env
   cp frontend/.env.example frontend/.env
   ```

5. **Start Development Environment**
   ```bash
   # Using Docker (recommended)
   make docker-dev
   
   # Or locally
   make dev
   ```

### Development

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:8080
- **Logparser Microservice**: http://localhost:8001
- **Database**: PostgreSQL on localhost:5432

## 🔐 Default Login Credentials

The application comes with pre-configured users for testing:

- **Admin**: `admin@autolog.com` / `admin123`
- **Manager**: `manager@autolog.com` / `manager123`
- **Responder**: `responder@autolog.com` / `responder123`
- **Viewer**: `viewer@autolog.com` / `viewer123`

## 🧠 Enhanced ML Log Parser

The logparser microservice provides intelligent log parsing using 14+ machine learning algorithms:

### **Available ML Algorithms:**
- **Drain**: Hierarchical clustering for structured logs
- **Spell**: Spell-based log parsing for duplicate detection
- **IPLoM**: Iterative partitioning for log mining
- **LogCluster**: Clustering-based log parsing
- **LenMa**: Length-based log parsing
- **LFA**: Log format analyzer
- **LKE**: Log key extraction
- **LogMine**: Multi-level log parsing
- **LogSig**: Signature-based log parsing
- **Logram**: N-gram based log parsing
- **SLCT**: Simple log clustering toolkit
- **ULP**: Unsupervised log parsing
- **Brain**: Neural network-based parsing
- **AEL**: Automated event log parsing

### **Supported Log Types:**
- **JSON Logs**: Structured JSON with automatic field extraction
- **Structured Logs**: Syslog, application logs, system logs
- **Web Server Logs**: Apache, Nginx access logs
- **Container Logs**: Docker, Kubernetes logs
- **Security Logs**: Authentication, authorization events
- **Mixed Content**: Hybrid JSON and unstructured logs
- **Unstructured Logs**: Free-form text with intelligent parsing

### **Features:**
- **Intelligent Algorithm Selection**: Automatically chooses the best ML algorithm based on log characteristics
- **Field Extraction**: Extracts timestamps, log levels, IP addresses, HTTP fields, and more
- **High Performance**: Processes 7,000+ entries per second
- **Robust Fallback**: Multiple fallback mechanisms for edge cases
- **Comprehensive Testing**: Full test suite with real-world scenarios

### **Testing the Logparser:**

```bash
# Run all tests
cd logparser_service
python3 test_all.py

# Test specific components
python3 test_all.py --ml          # Test ML parser functionality
python3 test_all.py --microservice # Test microservice API
python3 test_all.py --real-world  # Test real-world scenarios
python3 test_all.py --performance # Test performance with large datasets

# Test microservice health
curl http://localhost:8001/health
```

## 🏥 Health Checks & Startup Order

The application uses Docker's native health checks to ensure proper startup order:

1. **Database** starts first with PostgreSQL health checks
2. **Logparser Microservice** starts with health endpoint
3. **Backend** waits for database and logparser to be healthy, then starts
4. **Frontend** waits for backend to be healthy before starting

### Health Endpoints

**Backend Health**: `http://localhost:8080/health`
```json
{
  "status": "ok",
  "timestamp": "2024-01-01T12:00:00Z",
  "version": "1.0.0",
  "services": {
    "database": {
      "status": "ok",
      "error": null
    }
  }
}
```

**Logparser Health**: `http://localhost:8001/health`
```json
{
  "status": "healthy",
  "service": "logparser"
}
```

### Health Check Commands

```bash
# Check all services
make health

# Check specific services
make health-backend
make health-frontend
make health-db

# Test health endpoint with detailed output
make health-test
make health-test-docker
```

## 🌱 Initial Data Configuration

The application automatically seeds the database with initial users when starting in development mode. You can customize this data by editing the JSON files in `backend/data/`:

- `backend/data/initial-users.json` - Initial user accounts

See `backend/data/README.md` for detailed configuration options.

## 📚 Available Scripts

- `npm run dev` - Start both frontend and backend in development mode
- `npm run build` - Build both frontend and backend for production
- `npm run test` - Run tests for both frontend and backend
- `npm run lint` - Run linting for frontend

## 🔧 Configuration

### Environment Variables

#### Backend (.env)
```
DATABASE_URL="postgresql://username:password@localhost:5432/autolog"
JWT_SECRET="your-jwt-secret"
PORT=8080
ENV=development
LOGPARSER_URL="http://localhost:8001"
```

#### Frontend (.env)
```
VITE_API_URL=http://localhost:8080
VITE_WS_URL=ws://localhost:8080
```

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🆘 Support

For support and questions, please open an issue in the GitHub repository or contact the development team. 